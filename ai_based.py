# -*- coding: utf-8 -*-
"""AI BASED

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18LwSAv3zSSVTjweGQAhfhyfJw9bw5JdO
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install necessary NLP and file processing libraries
!pip install spacy scikit-learn pandas PyMuPDF python-docx
!python -m spacy download en_core_web_sm

import fitz  # PyMuPDF
from docx import Document
import os

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file."""
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

def extract_text_from_docx(docx_path):
    """Extracts text from a Word (.docx) file."""
    doc = Document(docx_path)
    return "\n".join([para.text for para in doc.paragraphs])

# --- ACTION: Replace these paths with the actual locations in your Google Drive ---
# Example: '/content/drive/MyDrive/YourFolder/my_resume.pdf'
resume_path = 'path_to_your_resume.docx'
job_desc_path = 'path_to_your_job_description.pdf'

# Load the text (Handling both PDF and DOCX)
try:
    if resume_path.endswith('.docx'):
        resume_raw = extract_text_from_docx(resume_path)
    else:
        resume_raw = extract_text_from_pdf(resume_path)

    if job_desc_path.endswith('.docx'):
        job_raw = extract_text_from_docx(job_desc_path)
    else:
        job_raw = extract_text_from_pdf(job_desc_path)

    print("Successfully extracted text from both files!")
except Exception as e:
    print(f"Error loading files: {e}. Check if the file paths are correct.")

# Install and Load NLP models
!pip install spacy scikit-learn pandas requests
!python -m spacy download en_core_web_sm

import pandas as pd
import requests
import spacy
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nlp = spacy.load("en_core_web_sm")
print("Setup Complete!")

import pandas as pd
import random

# Detailed Resume Data Generation
res_categories = ["IT", "Healthcare", "Finance", "Engineering"]
degrees = ["BSc", "MSc", "PhD", "Diploma"]
certs = ["AWS Certified", "PMP", "CPA", "Cisco CCNA", "Google Data Analytics", "N/A"]

resume_list = []
for i in range(550):
    cat = random.choice(res_categories)
    cgpa = round(random.uniform(2.5, 4.0), 2)
    exp = random.randint(0, 10)

    # Text format as if it was extracted from a PDF/DOCX
    text_content = f"""
    Category: {cat}
    Skills: {cat}_Skill_A, {cat}_Skill_B, Communication, Teamwork
    Degree: {random.choice(degrees)} in {cat}
    CGPA: {cgpa}
    Experience: {exp} years
    Certificates: {random.choice(certs)}
    """

    resume_list.append({
        'Resume_ID': i + 1,
        'Category': cat,
        'Resume_Text': text_content,
        'Candidate_CGPA': cgpa,
        'Candidate_Degree': random.choice(degrees),
        'Candidate_Experience': exp
    })

df_resumes = pd.DataFrame(resume_list)
print(f"Generated {len(df_resumes)} detailed resumes.")

# Detailed Job Description Generation
job_list = []
for i in range(1550):
    cat = random.choice(res_categories)
    min_cgpa = random.choice([2.0, 2.5, 3.0, 3.5])
    req_degree = random.choice(["BSc", "MSc", "PhD"])
    vacancies = random.randint(1, 5)

    job_desc = f"""
    Title: Senior {cat} Specialist
    Location: Addis Ababa / Mekelle
    Required Skills: {cat}_Skill_A, {cat}_Skill_C, Leadership
    Education: {req_degree} Required
    Minimum CGPA: {min_cgpa}
    Experience Needed: {random.randint(1, 5)} years
    Vacancies: {vacancies}
    """

    job_list.append({
        'Job_ID': i + 1,
        'Job_Title': f"{cat} Specialist",
        'Job_Description': job_desc,
        'Required_CGPA': min_cgpa,
        'Required_Degree': req_degree,
        'Location': random.choice(["Mekelle", "Addis Ababa", "Remote"])
    })

df_jobs = pd.DataFrame(job_list)
print(f"Generated {len(df_jobs)} detailed job descriptions.")

from google.colab import files

# 1. Save the Detailed Resume Dataset (550+ records)
df_resumes.to_csv('mekelle_university_resumes.csv', index=False)

# 2. Save the Detailed Job Description Dataset (1500+ records)
df_jobs.to_csv('mekelle_university_jobs.csv', index=False)

print("Finalizing files for download...")

# 3. Trigger the browser download for both files
files.download('mekelle_university_resumes.csv')
files.download('mekelle_university_jobs.csv')

print("Success! Check your Downloads folder for the CSV files.")

import spacy
from sklearn.metrics.pairwise import cosine_similarity
# You will need a transformer-based model like Sentence-BERT for the next phase
!pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')

def get_advanced_match(resume_text, job_text):
    # This implements Section 3.4 & 3.5 of your thesis
    embeddings = model.encode([resume_text, job_text])
    score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    return round(score * 100, 2)

# Test the new model
print(f"Advanced Match Score: {get_advanced_match(df_resumes['Resume_Text'][0], df_jobs['Job_Description'][0])}%")

import pandas as pd
import re

# Load your dataframes (df_jobs, df_resumes)
# If they are already in memory, skip loading.

def clean_text(text):
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # remove extra spaces
    text = text.strip()
    return text

df_jobs['Job_Description_Clean'] = df_jobs['Job_Description'].apply(clean_text)
df_resumes['Resume_Text_Clean'] = df_resumes['Resume_Text'].apply(clean_text)

print(df_jobs.head())
print(df_resumes.head())

def extract_skills(text):
    match = re.search(r"skills:(.*?)(degree|cgpa|experience|certificates)", text)
    if match:
        skills_raw = match.group(1)
        return [s.strip() for s in skills_raw.split(",")]
    return []

def extract_degree(text):
    match = re.search(r"degree:\s*(.*?)\s*cgpa", text)
    return match.group(1).strip() if match else None

def extract_cgpa(text):
    match = re.search(r"cgpa:\s*([0-9.]+)", text)
    return float(match.group(1)) if match else None

def extract_experience(text):
    match = re.search(r"experience:\s*([0-9]+)", text)
    return int(match.group(1)) if match else None

# Apply on resume text
df_resumes["Extracted_Skills"] = df_resumes["Resume_Text_Clean"].apply(extract_skills)
df_resumes["Extracted_Degree"] = df_resumes["Resume_Text_Clean"].apply(extract_degree)
df_resumes["Extracted_CGPA"] = df_resumes["Resume_Text_Clean"].apply(extract_cgpa)
df_resumes["Extracted_Experience"] = df_resumes["Resume_Text_Clean"].apply(extract_experience)

# Job dataset skill extraction (simplified)
def extract_job_skills(text):
    match = re.search(r"required skills:(.*?)(education|min)", text)
    if match:
        skills_raw = match.group(1)
        return [s.strip() for s in skills_raw.split(",")]
    return []

df_jobs["Extracted_Skills"] = df_jobs["Job_Description_Clean"].apply(extract_job_skills)

print(df_resumes.head())
print(df_jobs.head())

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")

import numpy as np

df_resumes["Embedding"] = df_resumes["Resume_Text_Clean"].apply(lambda x: model.encode(x))
df_jobs["Embedding"] = df_jobs["Job_Description_Clean"].apply(lambda x: model.encode(x))

print("Embedding example:", df_resumes["Embedding"].iloc[0][:10])

from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(resume_vec, job_vec):
    return cosine_similarity(
        resume_vec.reshape(1, -1),
        job_vec.reshape(1, -1)
    )[0][0]

score = calculate_similarity(
    df_resumes["Embedding"].iloc[0],
    df_jobs["Embedding"].iloc[0]
)

print("Similarity:", score)

def skill_gap(job_skills, resume_skills):
    job_skills = set([s.lower() for s in job_skills])
    resume_skills = set([s.lower() for s in resume_skills])

    missing = job_skills - resume_skills
    matched = job_skills & resume_skills
    gap_percentage = len(missing) / len(job_skills) if job_skills else 0

    return matched, missing, round(gap_percentage, 2)

matched, missing, gap = skill_gap(
    df_jobs["Extracted_Skills"].iloc[0],
    df_resumes["Extracted_Skills"].iloc[0]
)

print("Matched:", matched)
print("Missing:", missing)
print("Skill Gap %:", gap)

def match_jobs_for_resume(resume_index, top_k=5):
    resume_vec = df_resumes["Embedding"].iloc[resume_index]
    resume_skills = df_resumes["Extracted_Skills"].iloc[resume_index]

    results = []

    for i in range(len(df_jobs)):
        job_vec = df_jobs["Embedding"].iloc[i]
        job_skills = df_jobs["Extracted_Skills"].iloc[i]

        sim = calculate_similarity(resume_vec, job_vec)
        matched, missing, gap = skill_gap(job_skills, resume_skills)

        results.append({
            "Job_ID": df_jobs["Job_ID"].iloc[i],
            "Job_Title": df_jobs["Job_Title"].iloc[i],
            "Similarity": round(sim, 3),
            "Matched_Skills": list(matched),
            "Missing_Skills": list(missing),
            "Skill_Gap": gap
        })

    # Sort by similarity score (highest first)
    results = sorted(results, key=lambda x: x["Similarity"], reverse=True)
    return pd.DataFrame(results[:top_k])

match_jobs_for_resume(0)

# Commented out IPython magic to ensure Python compatibility.
# !pip install streamlit
# 
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# 
# st.title("AI Job Matching & Skill Gap Analysis System")
# 
# resume_id = st.number_input("Enter Resume ID:", min_value=1, max_value=len(df_resumes))
# 
# if st.button("Match Jobs"):
#     result = match_jobs_for_resume(resume_id - 1)
#     st.dataframe(result)

with open("app.py", "w") as f:
    f.write("")

import pandas as pd
import random

# --------------------------
# Regenerate JOBS
# --------------------------
res_categories = ["IT", "Healthcare", "Finance", "Engineering"]

job_list = []
for i in range(1550):
    cat = random.choice(res_categories)
    min_cgpa = random.choice([2.0, 2.5, 3.0, 3.5])
    req_degree = random.choice(["BSc", "MSc", "PhD"])
    vacancies = random.randint(1, 5)

    job_desc = f"""
    Title: Senior {cat} Specialist
    Location: Addis Ababa / Mekelle
    Required Skills: {cat}_Skill_A, {cat}_Skill_C, Leadership
    Education: {req_degree} Required
    Minimum CGPA: {min_cgpa}
    Experience Needed: {random.randint(1, 5)} years
    Vacancies: {vacancies}
    """

    job_list.append({
        'Job_ID': i + 1,
        'Job_Title': f"{cat} Specialist",
        'Job_Description': job_desc,
        'Required_CGPA': min_cgpa,
        'Required_Degree': req_degree,
        'Location': random.choice(["Mekelle", "Addis Ababa", "Remote"])
    })

df_jobs = pd.DataFrame(job_list)

# --------------------------
# Regenerate RESUMES
# --------------------------
degrees = ["BSc", "MSc", "PhD", "Diploma"]
certs = ["AWS Certified", "PMP", "CPA", "Cisco CCNA", "Google Data Analytics", "N/A"]

resume_list = []
for i in range(550):
    cat = random.choice(res_categories)
    cgpa = round(random.uniform(2.5, 4.0), 2)
    exp = random.randint(0, 10)

    text_content = f"""
    Category: {cat}
    Skills: {cat}_Skill_A, {cat}_Skill_B, Communication, Teamwork
    Degree: {random.choice(degrees)} in {cat}
    CGPA: {cgpa}
    Experience: {exp} years
    Certificates: {random.choice(certs)}
    """

    resume_list.append({
        'Resume_ID': i + 1,
        'Category': cat,
        'Resume_Text': text_content,
        'Candidate_CGPA': cgpa,
        'Candidate_Degree': random.choice(degrees),
        'Candidate_Experience': exp
    })

df_resumes = pd.DataFrame(resume_list)

print("Datasets regenerated!")

df_resumes.to_csv("cleaned_resumes.csv", index=False)
df_jobs.to_csv("cleaned_jobs.csv", index=False)

print("Files saved successfully!")

df_resumes = pd.read_csv("cleaned_resumes.csv")
df_jobs = pd.read_csv("cleaned_jobs.csv")

print("Files loaded successfully!")

import re
import pandas as pd

# Simple text cleaning function
def clean_text(text):
    text = str(text).lower()  # lowercase
    text = re.sub(r'\n', ' ', text)  # remove newlines
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # remove special chars
    text = re.sub(r'\s+', ' ', text).strip()  # remove extra spaces
    return text

# Apply cleaning
df_resumes["Cleaned_Text"] = df_resumes["Resume_Text"].apply(clean_text)
df_jobs["Cleaned_Job_Description"] = df_jobs["Job_Description"].apply(clean_text)

print("Text cleaned successfully!")

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(stop_words="english")

resume_vectors = vectorizer.fit_transform(df_resumes["Cleaned_Text"])
job_vectors = vectorizer.transform(df_jobs["Cleaned_Job_Description"])

print("TF-IDF embeddings generated!")

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")

resume_vectors = model.encode(df_resumes["Cleaned_Text"], convert_to_tensor=True)
job_vectors = model.encode(df_jobs["Cleaned_Job_Description"], convert_to_tensor=True)

print("SBERT embeddings generated!")

from sklearn.metrics.pairwise import cosine_similarity

similarity_matrix = cosine_similarity(resume_vectors, job_vectors)

print(similarity_matrix.shape)

import numpy as np

best_match_index = np.argmax(similarity_matrix, axis=1)
best_match_score = np.max(similarity_matrix, axis=1)

df_resumes["Best_Job_Index"] = best_match_index
df_resumes["Match_Score"] = best_match_score

df_resumes.head()

# Map best job for each resume
df_resumes["Best_Job_Title"] = df_resumes["Best_Job_Index"].apply(
    lambda x: df_jobs.iloc[x]["Job_Title"]
)

df_resumes["Best_Job_Description"] = df_resumes["Best_Job_Index"].apply(
    lambda x: df_jobs.iloc[x]["Job_Description"]
)

df_resumes["Best_Job_CGPA"] = df_resumes["Best_Job_Index"].apply(
    lambda x: df_jobs.iloc[x]["Required_CGPA"]
)

df_resumes["Best_Job_Degree"] = df_resumes["Best_Job_Index"].apply(
    lambda x: df_jobs.iloc[x]["Required_Degree"]
)

df_resumes.head()

import re

def extract_skills(text):
    skills_line = re.findall(r"Skills:(.*)", text)
    if skills_line:
        return [s.strip() for s in skills_line[0].split(",")]
    return []

df_resumes["Extracted_Skills"] = df_resumes["Resume_Text"].apply(extract_skills)

def job_extract_skills(text):
    skills_line = re.findall(r"Required Skills:(.*)", text)
    if skills_line:
        return [s.strip() for s in skills_line[0].split(",")]
    return []

df_jobs["Extracted_Skills"] = df_jobs["Job_Description"].apply(job_extract_skills)

def skill_gap(resume_skills, job_skills):
    resume_set = set(resume_skills)
    job_set = set(job_skills)

    missing = job_set - resume_set
    gap_percent = round((len(missing) / len(job_set)) * 100, 2) if len(job_set) > 0 else 0

    return list(missing), gap_percent

df_resumes["Skill_Gap"] = df_resumes.apply(
    lambda row: skill_gap(
        row["Extracted_Skills"],
        df_jobs.iloc[row["Best_Job_Index"]]["Extracted_Skills"]
    ),
    axis=1
)

df_resumes["Missing_Skills"] = df_resumes["Skill_Gap"].apply(lambda x: x[0])
df_resumes["Gap_Percentage"] = df_resumes["Skill_Gap"].apply(lambda x: x[1])

df_resumes[["Resume_ID", "Best_Job_Title", "Missing_Skills", "Gap_Percentage"]].head()

df_resumes.to_csv("resume_job_matching_results.csv", index=False)
df_jobs.to_csv("processed_jobs.csv", index=False)

print("Files saved successfully!")

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Distribution of Match Scores
plt.figure(figsize=(8,5))
sns.histplot(df_resumes["Match_Score"], bins=30)
plt.title("Distribution of Resumeâ€“Job Match Scores")
plt.xlabel("Similarity Score")
plt.ylabel("Count")
plt.show()

# 2. Skill Gap Percentage
plt.figure(figsize=(8,5))
sns.histplot(df_resumes["Gap_Percentage"], bins=30)
plt.title("Distribution of Skill Gap Percentage")
plt.xlabel("Skill Gap %")
plt.ylabel("Number of Candidates")
plt.show()

# 3. Experience vs Match Score
plt.figure(figsize=(8,5))
sns.scatterplot(x=df_resumes["Candidate_Experience"], y=df_resumes["Match_Score"])
plt.title("Experience vs Match Score")
plt.xlabel("Years of Experience")
plt.ylabel("Match Score")
plt.show()

import streamlit as st
import pandas as pd
import joblib

st.title("AI-Based Job Matching & Skill Gap Analysis")

# Load saved results
df = pd.read_csv("resume_job_matching_results.csv")

resume_id = st.number_input("Enter Resume ID:", min_value=1, max_value=len(df))

if st.button("Find Best Job"):
    row = df[df["Resume_ID"] == resume_id].iloc[0]

    st.subheader("Best Job Match")
    st.write("**Job Title:**", row["Best_Job_Title"])
    st.write("**Match Score:**", row["Match_Score"])
    st.write("**Skill Gap %:**", row["Gap_Percentage"])
    st.write("**Missing Skills:**", row["Missing_Skills"])

!streamlit run app.py & npx localtunnel --port 8501

!pip install streamlit

!pip install streamlit pyngrok

from pyngrok import ngrok

ngrok.set_auth_token("ngrok config add-authtoken 3ADkEF7Yp0OPy8wQd92RD7uPiPu_6tkTisKkjjWca9ZoigPXL")

!streamlit run /content/drive/MyDrive/app.py --server.port 8501 --server.headless true &

from google.colab import drive
drive.mount('/content/drive')

!ls /content/drive/MyDrive

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive

!streamlit run app.py

!pip install pyngrok
from pyngrok import ngrok

ngrok.set_auth_token("ngrok config add-authtoken 3ADkEF7Yp0OPy8wQd92RD7uPiPu_6tkTisKkjjWca9ZoigPXL")

from google.colab import drive
drive.mount('/content/drive')

!zip -r jobmatching.zip /content/drive/MyDrive/MyColabProject